前言：
机器学习的一次小作业。

# 不同梯度下降算法的比较

## 目的及概述：

选择线性回归+MSE作为损失函数，选择Boston房价数据集，比较GD、随机GD、小批量SGD算法的性能（包括收敛时间、训练误差），同时对学习率、小批量的大小这两个变量的进行简单讨论。

## 代码实现：

### 数据集的引入以及标准化操作：

由于sklearn库中已将Boston房价这一数据集删除，因此我选择在cmu官网数据集中导入，导入之后将数据集拆分成训练集和测试集，然后进行特征标准化和添加偏置项操作。以下是代码实现：

``` python
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 加载数据集并拆分为训练集和测试集
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

X, y = data, target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 特征标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 添加偏置项
X_train_scaled = np.c_[np.ones(X_train.shape[0]), X_train_scaled]
X_test_scaled = np.c_[np.ones(X_test.shape[0]), X_test_scaled]
```

### 算法部分：

算法部分首先需要实现损失函数和梯度函数的定义，方便后面直接调用，下面是代码实现：

```python
# 定义损失函数和梯度函数
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def gradient(X, y, theta):
    n_samples = X.shape[0]
    y_pred = X.dot(theta)
    grad = X.T.dot(y_pred - y) / n_samples
    return grad
```

然后，分别实现梯度下降（GD）、随机梯度下降（SGD）、小规模随机梯度下降（mini-batch SGD）算法，下面是代码实现：

```python
# 标准梯度下降算法
def gradient_descent(X, y, lr, n_iters):
    n_samples, n_features = X.shape
    theta = np.zeros(n_features)
    loss_history = []

    # 开始迭代
    for i in range(n_iters):
        # 计算梯度
        gradients = np.dot(X.T, np.dot(X, theta) - y) / n_samples

        # 更新模型参数
        theta -= lr * gradients

        # 记录损失函数值
        loss = mse(y, np.dot(X, theta))
        loss_history.append(loss)

    return theta, loss_history

# 随机梯度下降算法
def stochastic_gradient_descent(X, y, lr, n_iters):
    n_samples, n_features = X.shape
    theta = np.zeros(n_features)
    loss_history = []

    for i in range(n_iters):
        idx = np.random.randint(n_samples)
        xi, yi = X[idx], y[idx]
        grad = gradient(xi, yi, theta)
        theta = theta - lr * grad
        y_pred = X.dot(theta)
        loss = mse(y, y_pred)
        loss_history.append(loss)

    return theta, loss_history

# 小批量随机梯度算法
def mini_batch_stochastic_gradient_descent(X, y, lr, n_iters, batch_size):
    n_samples, n_features = X.shape
    theta = np.zeros(n_features)
    loss_history = []

    # 开始迭代
    for i in range(n_iters):
        # 打乱训练数据
        permutation = np.random.permutation(n_samples)
        X = X[permutation]
        y = y[permutation]

        # 遍历每个小批量
        for j in range(0, n_samples, batch_size):
            X_batch = X[j:j+batch_size]
            y_batch = y[j:j+batch_size]

            # 计算梯度
            gradients = np.dot(X_batch.T, np.dot(X_batch, theta) - y_batch) / batch_size

            # 更新模型参数
            theta -= lr * gradients

        # 记录损失函数值
        loss = mse(y, np.dot(X, theta))
        loss_history.append(loss)

    return theta, loss_history
```

### 代码执行及时间计算

首先选择参数，这里面的参数有学习率（lr）、迭代次数（n_iters）和小批量选择大小（batch_size），我先默认选成0.01、2000、16，如下所示：

```python
lr = 0.01
n_iters = 2000
batch_size = 16
```

然后在运行的同时进行执行时间的计算，如下所示：

```python
import time

# 记录算法执行时间
start_time = time.time()
theta_batch, loss_batch = gradient_descent(X_train_scaled, y_train, lr, n_iters)
batch_time = time.time() - start_time

start_time = time.time()
theta_stochastic, loss_stochastic = stochastic_gradient_descent(X_train_scaled, y_train, lr, n_iters)
stochastic_time = time.time() - start_time

start_time = time.time()
theta_mini_batch, loss_mini_batch = mini_batch_stochastic_gradient_descent(X_train_scaled, y_train, lr, n_iters, batch_size)
mini_batch_time = time.time() - start_time

# 打印算法执行时间
print(f'GD: {batch_time:.4f} s')
print(f'SGD: {stochastic_time:.4f} s')
print(f'Mini-batch SGD: {mini_batch_time:.4f} s')
```

### 结果可视化

随后使用matplotlib库进行可视化图像的绘制，以迭代次数为横坐标，损失值为纵坐标，绘制图像，比较三种梯度下降算法。以下是代码实现：

```python
import matplotlib.pyplot as plt

plt.plot(loss_batch, label='GD')
plt.plot(loss_stochastic, label='SGD')
plt.plot(loss_mini_batch, label='Mini-batch SGD')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

## 结果分析及总结

在上述参数的情况下运行代码，得到的结果为：

```python
GD: 0.0421 s
SGD: 0.0431 s
Mini-batch SGD: 0.3517 s
```

绘制的图像为：


可以看到小批量SGD算法用最小的迭代次数达到了收敛的结果，但其每一次迭代所需要的时间最长。

标准GD算法收敛速度次之，时间与SGD不相上下，应该是这个数据集不够大，因此标准GD比SGD占优势。

SGD算法更新迭代速度很快，但其准确度不高，在迭代次数2000的时候还与收敛有一段距离。

下面将lr和batch_size这两个参数分别进行修改查看结果。首先修改lr。



现将lr减小为0.005，得到结果如下：

```python
GD: 0.0484 s
SGD: 0.0459 s
Mini-batch SGD: 0.3916 s
```


下面依次将其改为0.05、0.1、0.2、0.5（横坐标迭代次数做了相应调整）：

0.05时：

```python
GD: 0.0459 s
SGD: 0.0446 s
Mini-batch SGD: 0.3831 s
```


0.1时：

```python
GD: 0.0075 s
SGD: 0.0065 s
Mini-batch SGD: 0.0547 s
```


0.2时：

```python
GD: 0.0025 s
SGD: 0.0030 s
Mini-batch SGD: 0.0190 s
```

0.5时：

```python
GD: 0.0030 s
SGD: 0.0020 s
Mini-batch SGD: 0.0210 s
```


**综合以上可以看出，lr的值越大，达到收敛所需要的迭代次数就越少。随着lr值增大，GD依旧稳定，而SGD和Mini-batch SGD会出现不稳定的情况，尤其到了0.5的时候小批量SGD会出现异常值。因此lr不能过大。**



下面研究batch_size大小的变化（lr定为0.005）：

由于batch_size只与小规模SGD的结果有关，因此我只绘制小规模SGD的图像，令batch_size从4到128呈指数变化，结果如下图所示：

可以看到batch_size取值越小，达到损失值随迭代次数下降越快，越容易达到收敛，但是随之时间成本也会提高，因此要选择合适的batch_size大小。

